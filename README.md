# Scraper
this is a web scraper i build using aria2c moduleand sub process module

every file uses subprocess and is multi threaded to process large amounts of data 

#####################################################################################

-> Worker.py       is responsible for multi threading
The following code is a Python script that defines a function Worker(). The Worker() function takes a command as its argument and performs the following operations:

Opens a log file named Output_log.txt in binary mode for appending data and stores it in the f variable.
Opens a log file named Command_log.txt in append mode and writes the current command being executed to the file.
Executes the command using the subprocess.Popen() function. The stdout parameter is set to the f variable so that any output generated by the command is written to the Output_log.txt file.
Appends the created process and the opened file to a list called pool.
The subprocess.Popen() function spawns a new process and returns a Popen object. The Popen object can be used to interact with the newly created process.

The purpose of the Worker() function is to execute shell commands and write their output to a log file. The pool list maintains a list of all the Popen objects and log files that have been created by the function. This can be used to keep track of all the running processes and their output files.

#####################################################################################
-> download.py     is responsible for downloading links

The following code is a Python script that imports the Worker function from a module called Worker. It defines a variable called chunk_size with a value of 100 MB.

The code then attempts to execute the aria2c command using the Worker function. The aria2c command is a command-line download manager utility that can download files from the internet using multiple connections and multiple sources.

The aria2c command takes the following options and arguments:

-x8, -s8, and -j8 are options that specify the number of connections to be used for downloading the file. In this case, 8 connections are being used for each server, and the maximum number of connections is also set to 8.
--split=8 and --min-split-size=100M are options that control the size of the individual chunks of the file that are downloaded simultaneously. In this case, 8 chunks of at least 100 MB each are being used.
--dir=/path/to/download/location is an option that specifies the directory where the downloaded file should be saved.
"http://example.com/file.zip" is the URL of the file to be downloaded.
The sys.argv variable is used to get the command-line arguments passed to the script. In this case, the first argument (sys.argv[1]) is expected to be the path to the directory where the downloaded file should be saved, and the second argument (sys.argv[2]) is expected to be the URL of the file to be downloaded.

If an exception occurs during the execution of the Worker function, it is caught and printed to the console using the print() statement inside the except block.

#####################################################################################

-> link_extract.py is responsible for creating single link bundle

The following code defines three functions that read data from input files and save the extracted data as JSON to an output file.

The input file paths are defined in a list called input_files, and the output file path is defined in a variable called output_file.

The fromJson() function reads each input file as JSON, extracts the permalink values from the items field, and adds them to a list called links. The links list is returned by the function.

The fromTXT() function reads each input file as plain text, splits the text into individual lines, and adds any line that starts with "https://" to a list called links. The links list is returned by the function.

The save() function takes a list called links as input and saves it as JSON to the output file specified by output_file.

The last line of code calls the fromTXT() function to extract the data from the input files and saves the data to the output file using the save() function.

#####################################################################################
-> scrape.py       is resposible to download every link from link bundle
The following code is a Python script that defines a class scraper which contains a single method download(). The scraper class constructor initializes the object by reading data from a JSON file named permalink.json and storing it in a class attribute self.data. The self.operation_length attribute is also initialized to the length of the links list in the self.data attribute. Finally, the length of the operation is printed to the console.

The download() method loops through the links list in the self.data attribute and performs the following operations for each link:

Extracts the name of the file to be downloaded from the link using list slicing.
Prints a message to the console indicating the number and name of the file being downloaded.
Creates a new directory in the downloads folder with the extracted name of the file.
Pauses for 0.1 seconds.
Runs the download.py script using the Worker class imported as wk. The script takes two arguments: the first argument is the path to the newly created directory, and the second argument is the link to the file to be downloaded.
If an exception occurs during the execution of the try block, it is caught and printed to the console using the print() statement inside the except block.
Finally, the code creates an instance of the scraper class called main and calls the download() method on it. This starts the downloading process.

#####################################################################################

-> site_call.py    is used to check status code of a link

This script makes a URL request to the website provided as a command line argument (sys.argv[1]) using the urllib module in Python.

The with statement is used to open the URL connection, and the getcode() method is called to get the HTTP status code returned by the server. The URL and the status code are then printed to the console.

If an HTTPError is encountered (i.e., if the server returns a non-200 status code), the exception is caught and the HTTP error code and reason are printed to the console.

If a URLError is encountered (i.e., if the URL is malformed or cannot be reached), the exception is caught and the reason for the error is printed to the console.

If any other type of exception is encountered, the exception is caught and the error message is printed to the console along with the URL.

#####################################################################################

-> sitemap_loader.py used to check for the site map using site_call.py of a website

The code is written in Python and is intended to crawl a website for its sitemap and robots.txt files. It uses a module called "Worker" to execute a Python script called "site_call.py" with a set of URLs passed as arguments.

The "sys" module is imported to access command-line arguments passed to the script. The first argument is used as the base URL for the website to be crawled. The "url_list" variable contains a list of URLs that are commonly used for sitemaps and robots.txt files. These URLs are appended to the base URL to form the final URLs that will be crawled.

The "Worker" module is called for each URL in the "url_list" variable to execute the "site_call.py" script with the URL passed as an argument. The "site_call.py" script likely contains code to download the URL and extract the necessary information from the sitemap or robots.txt file.

Note that the comments in the code provide additional information about the purpose of each URL in the "url_list" variable.

#####################################################################################

-> html_read.py used to extract data from downloaded sites

The purpose of this code is to extract text data from downloaded HTML files and write it to a text file.

The code first loads a list of links from a JSON file named "permalink/permalink.json" and extracts the path from each link. It then opens a text file named "dataset/studyhelpinghand.txt" in append mode, which means that any new data written to the file will be appended to the existing data.

For each path extracted from the links, the code attempts to open an HTML file located at "downloads/{path}/index.html". If successful, the HTML file is parsed using the BeautifulSoup library and all the text data (from p and li tags) is extracted and stored in a list called lines. The code then writes each line of text to the text file and prints a message indicating the completion of the process.

Any errors encountered during the process will be caught by the try-except block, and an error message will be printed.

After all the data has been written to the text file, the code closes the file.
